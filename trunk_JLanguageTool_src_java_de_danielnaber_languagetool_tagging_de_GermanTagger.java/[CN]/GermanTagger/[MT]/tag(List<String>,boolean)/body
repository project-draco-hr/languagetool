{
  String[] taggerTokens;
  boolean firstWord=true;
  List<AnalyzedTokenReadings> tokenReadings=new ArrayList<AnalyzedTokenReadings>();
  int pos=0;
  if (morfologik == null) {
    setFileName();
    morfologik=new Lametyzator();
  }
  if (manualTagger == null) {
    manualTagger=new ManualTagger(Tools.getStream(USER_DICT_FILENAME));
  }
  if (compoundTokenizer == null) {
    compoundTokenizer=new GermanCompoundTokenizer();
  }
  for (  String word : sentenceTokens) {
    List<AnalyzedGermanToken> l=new ArrayList<AnalyzedGermanToken>();
    taggerTokens=lexiconLookup(word);
    if (firstWord && taggerTokens == null && ignoreCase) {
      taggerTokens=lexiconLookup(word.toLowerCase());
      firstWord=false;
    }
    if (taggerTokens != null) {
      tagWord(taggerTokens,word,l);
    }
 else {
      if (!StringTools.isEmpty(word.trim())) {
        List<String> compoundParts=compoundTokenizer.tokenize(word);
        if (compoundParts.size() <= 1) {
          l.add(new AnalyzedGermanToken(word,null,null));
        }
 else {
          String lastPart=compoundParts.get(compoundParts.size() - 1);
          if (StringTools.startsWithUppercase(word)) {
            lastPart=StringTools.uppercaseFirstChar(lastPart);
          }
          taggerTokens=lexiconLookup(lastPart);
          if (taggerTokens != null) {
            tagWord(taggerTokens,word,l);
          }
 else {
            l.add(new AnalyzedGermanToken(word,null,null));
          }
        }
      }
 else {
        l.add(new AnalyzedGermanToken(word,null,null));
      }
    }
    tokenReadings.add(new AnalyzedGermanTokenReadings(l.toArray(new AnalyzedGermanToken[l.size()]),pos));
    pos+=word.length();
  }
  return tokenReadings;
}
