{
  String[] taggerTokens;
  final List<AnalyzedTokenReadings> tokenReadings=new ArrayList<AnalyzedTokenReadings>();
  int pos=0;
  if (morfologik == null) {
    setFileName();
    morfologik=new Lametyzator();
  }
  for (  final String word : sentenceTokens) {
    final List<AnalyzedToken> l=new ArrayList<AnalyzedToken>();
    String[] lowerTaggerTokens=null;
    taggerTokens=morfologik.stemAndForm(word);
    if (!word.equals(word.toLowerCase(csLocale))) {
      lowerTaggerTokens=morfologik.stemAndForm(word.toLowerCase(csLocale));
    }
    if (taggerTokens != null) {
      int i=0;
      while (i < taggerTokens.length) {
        final String lemma=taggerTokens[i];
        final String[] tagsArr=taggerTokens[i + 1].split("\\+");
        for (        final String currTag : tagsArr) {
          l.add(new AnalyzedToken(word,currTag,lemma,pos));
        }
        i+=2;
      }
    }
    if (lowerTaggerTokens != null) {
      int i=0;
      while (i < lowerTaggerTokens.length) {
        final String lemma=lowerTaggerTokens[i];
        final String[] tagsArr=lowerTaggerTokens[i + 1].split("\\+");
        for (        final String currTag : tagsArr) {
          l.add(new AnalyzedToken(word,currTag,lemma,pos));
        }
        i+=2;
      }
    }
    if (lowerTaggerTokens == null && taggerTokens == null) {
      l.add(new AnalyzedToken(word,null,pos));
    }
    pos+=word.length();
    tokenReadings.add(new AnalyzedTokenReadings(l.toArray(new AnalyzedToken[l.size()])));
  }
  return tokenReadings;
}
