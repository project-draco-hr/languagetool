{
  StringBuilder outputStr=new StringBuilder();
  List<String> sentences=sentenceTokenizer.tokenize(input);
  for (  String sentence : sentences) {
    List<String> tokens=tokenizer.tokenize(sentence);
    List<String> noWhitespaceTokens=getNoWhitespaceTokens(tokens);
    List<AnalyzedTokenReadings> aTokens=tagger.tag(noWhitespaceTokens);
    AnalyzedTokenReadings[] tokenArray=new AnalyzedTokenReadings[tokens.size() + 1];
    AnalyzedToken[] startTokenArray=new AnalyzedToken[1];
    int toArrayCount=0;
    AnalyzedToken sentenceStartToken=new AnalyzedToken("",JLanguageTool.SENTENCE_START_TAGNAME,null);
    startTokenArray[0]=sentenceStartToken;
    tokenArray[toArrayCount++]=new AnalyzedTokenReadings(startTokenArray,0);
    int startPos=0;
    int noWhitespaceCount=0;
    for (    String tokenStr : tokens) {
      AnalyzedTokenReadings posTag;
      if (isWord(tokenStr)) {
        posTag=aTokens.get(noWhitespaceCount);
        posTag.setStartPos(startPos);
        noWhitespaceCount++;
      }
 else {
        posTag=tagger.createNullToken(tokenStr,startPos);
      }
      tokenArray[toArrayCount++]=posTag;
      startPos+=tokenStr.length();
    }
    AnalyzedSentence finalSentence=new AnalyzedSentence(tokenArray);
    finalSentence=disambiguator.disambiguate(finalSentence);
    AnalyzedTokenReadings[] output=finalSentence.getTokens();
    for (int i=0; i < output.length; i++) {
      AnalyzedTokenReadings tokenReadings=output[i];
      List<String> readings=getAsStrings(tokenReadings);
      outputStr.append(String.join("|",readings));
      if (i < output.length - 1) {
        outputStr.append(' ');
      }
    }
  }
  assertEquals(expected,outputStr.toString());
}
