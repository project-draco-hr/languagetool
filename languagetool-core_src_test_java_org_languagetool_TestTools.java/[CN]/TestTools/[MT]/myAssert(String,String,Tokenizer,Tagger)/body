{
  final List<String> tokens=tokenizer.tokenize(input);
  final List<String> noWhitespaceTokens=new ArrayList<>();
  for (  final String token : tokens) {
    if (isWord(token)) {
      noWhitespaceTokens.add(token);
    }
  }
  final List<AnalyzedTokenReadings> output=tagger.tag(noWhitespaceTokens);
  final StringBuilder outputStr=new StringBuilder();
  for (final Iterator<AnalyzedTokenReadings> iter=output.iterator(); iter.hasNext(); ) {
    final AnalyzedTokenReadings tokenReadings=iter.next();
    final List<String> readings=new ArrayList<>();
    for (    AnalyzedToken analyzedToken : tokenReadings) {
      final StringBuilder readingStr=new StringBuilder();
      readingStr.append(analyzedToken.getToken());
      readingStr.append("/[");
      readingStr.append(analyzedToken.getLemma());
      readingStr.append(']');
      readingStr.append(analyzedToken.getPOSTag());
      readings.add(readingStr.toString());
    }
    Collections.sort(readings);
    outputStr.append(StringTools.listToString(readings,"|"));
    if (iter.hasNext()) {
      outputStr.append(" -- ");
    }
  }
  assertEquals(expected,outputStr.toString());
}
