{
  final List<RuleMatch> ruleMatches=new ArrayList<RuleMatch>();
  final AnalyzedTokenReadings[] tokens=text.getTokens();
  int pos=0;
  for (  AnalyzedTokenReadings tmpToken : tokens) {
    String token=tmpToken.getToken();
    if (tmpToken.isWhitespace()) {
    }
 else {
      final String origToken=token;
      final List<AnalyzedToken> readings=tmpToken.getReadings();
      if (readings != null && readings.size() > 0) {
        final String baseform=readings.get(0).getLemma();
        if (baseform != null) {
          token=baseform;
        }
 else {
          final String manualLookup=germanLemmatizer.getBaseform(origToken);
          if (manualLookup != null)           token=manualLookup;
        }
      }
      if (shouldNotAppearWord.containsKey(token)) {
        final RuleMatch otherMatch=shouldNotAppearWord.get(token);
        final String otherSpelling=otherMatch.getMessage();
        final String msg="'" + token + "' und '"+ otherSpelling+ "' sollten nicht gleichzeitig benutzt werden";
        final RuleMatch ruleMatch=new RuleMatch(this,pos,pos + origToken.length(),msg);
        ruleMatch.setSuggestedReplacement(otherSpelling);
        ruleMatches.add(ruleMatch);
      }
 else       if (relevantWords.containsKey(token)) {
        final String shouldNotAppear=relevantWords.get(token);
        final RuleMatch potentialRuleMatch=new RuleMatch(this,pos,pos + origToken.length(),token);
        shouldNotAppearWord.put(shouldNotAppear,potentialRuleMatch);
      }
    }
    pos+=tmpToken.getToken().length();
  }
  return toRuleMatchArray(ruleMatches);
}
