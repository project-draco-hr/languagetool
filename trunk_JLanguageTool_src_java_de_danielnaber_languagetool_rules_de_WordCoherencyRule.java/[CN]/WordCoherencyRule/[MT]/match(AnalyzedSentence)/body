{
  List<RuleMatch> ruleMatches=new ArrayList<RuleMatch>();
  AnalyzedTokenReadings[] tokens=text.getTokens();
  int pos=0;
  for (int i=0; i < tokens.length; i++) {
    String token=tokens[i].getToken();
    if (token.trim().equals("")) {
    }
 else {
      String origToken=token;
      List<AnalyzedToken> readings=tokens[i].getReadings();
      if (readings != null && readings.size() > 0) {
        String baseform=readings.get(0).getLemma();
        if (baseform != null) {
          token=baseform;
        }
 else {
          String manualLookup=germanLemmatizer.getBaseform(origToken);
          if (manualLookup != null)           token=manualLookup;
        }
      }
      if (shouldNotAppearWord.containsKey(token)) {
        RuleMatch otherMatch=(RuleMatch)shouldNotAppearWord.get(token);
        String otherSpelling=otherMatch.getMessage();
        String msg="<b>" + token + "</b> und <b>"+ otherSpelling+ "</b> sollten nicht gleichzeitig benutzt werden";
        RuleMatch ruleMatch=new RuleMatch(this,pos,pos + origToken.length(),msg);
        ruleMatch.setSuggestedReplacement(otherSpelling);
        ruleMatches.add(ruleMatch);
      }
 else       if (relevantWords.containsKey(token)) {
        String shouldNotAppear=(String)relevantWords.get(token);
        String msg=token;
        RuleMatch potentialRuleMatch=new RuleMatch(this,pos,pos + origToken.length(),msg);
        shouldNotAppearWord.put(shouldNotAppear,potentialRuleMatch);
      }
    }
    pos+=tokens[i].getToken().length();
  }
  return toRuleMatchArray(ruleMatches);
}
