{
  initializeIfRequired();
  boolean firstWord=true;
  List<AnalyzedTokenReadings> tokenReadings=new ArrayList<>();
  int pos=0;
  for (  String word : sentenceTokens) {
    List<AnalyzedToken> l=new ArrayList<>();
    List<TaggedWord> taggerTokens=getWordTagger().tag(word);
    if (firstWord && taggerTokens.size() == 0 && ignoreCase) {
      taggerTokens=getWordTagger().tag(word.toLowerCase());
      firstWord=false;
    }
    if (taggerTokens.size() > 0) {
      l.addAll(getAnalyzedTokens(taggerTokens,word));
    }
 else {
      if (!StringTools.isEmpty(word.trim())) {
        List<String> compoundParts=compoundTokenizer.tokenize(word);
        if (compoundParts.size() <= 1) {
          l.add(getNoInfoToken(word));
        }
 else {
          String lastPart=compoundParts.get(compoundParts.size() - 1);
          if (StringTools.startsWithUppercase(word)) {
            lastPart=StringTools.uppercaseFirstChar(lastPart);
          }
          List<TaggedWord> partTaggerTokens=getWordTagger().tag(lastPart);
          if (partTaggerTokens.size() > 0) {
            l.addAll(getAnalyzedTokens(partTaggerTokens,word,compoundParts));
          }
 else {
            l.add(getNoInfoToken(word));
          }
        }
      }
 else {
        l.add(getNoInfoToken(word));
      }
    }
    tokenReadings.add(new AnalyzedTokenReadings(l.toArray(new AnalyzedToken[l.size()]),pos));
    pos+=word.length();
  }
  return tokenReadings;
}
