{
  initializeIfRequired();
  String[] taggerTokens;
  boolean firstWord=true;
  final List<AnalyzedTokenReadings> tokenReadings=new ArrayList<>();
  int pos=0;
  final IStemmer morfologik=new DictionaryLookup(dictionary);
  for (  String word : sentenceTokens) {
    final List<AnalyzedGermanToken> l=new ArrayList<>();
    taggerTokens=lexiconLookup(word,morfologik);
    if (firstWord && taggerTokens == null && ignoreCase) {
      taggerTokens=lexiconLookup(word.toLowerCase(),morfologik);
      firstWord=false;
    }
    if (taggerTokens != null) {
      tagWord(taggerTokens,word,l);
    }
 else {
      if (!StringTools.isEmpty(word.trim())) {
        final List<String> compoundParts=compoundTokenizer.tokenize(word);
        if (compoundParts.size() <= 1) {
          l.add(new AnalyzedGermanToken(word,null,null));
        }
 else {
          String lastPart=compoundParts.get(compoundParts.size() - 1);
          if (StringTools.startsWithUppercase(word)) {
            lastPart=StringTools.uppercaseFirstChar(lastPart);
          }
          taggerTokens=lexiconLookup(lastPart,morfologik);
          if (taggerTokens != null) {
            tagWord(taggerTokens,word,l,compoundParts);
          }
 else {
            l.add(new AnalyzedGermanToken(word,null,null));
          }
        }
      }
 else {
        l.add(new AnalyzedGermanToken(word,null,null));
      }
    }
    tokenReadings.add(new AnalyzedGermanTokenReadings(l.toArray(new AnalyzedGermanToken[l.size()]),pos));
    pos+=word.length();
  }
  return tokenReadings;
}
