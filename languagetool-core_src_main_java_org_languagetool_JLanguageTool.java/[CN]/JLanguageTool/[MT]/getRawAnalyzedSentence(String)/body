{
  List<String> tokens=language.getWordTokenizer().tokenize(sentence);
  Map<Integer,String> softHyphenTokens=replaceSoftHyphens(tokens);
  List<AnalyzedTokenReadings> aTokens=language.getTagger().tag(tokens);
  if (language.getChunker() != null) {
    language.getChunker().addChunkTags(aTokens);
  }
  int numTokens=aTokens.size();
  int posFix=0;
  for (int i=1; i < numTokens; i++) {
    aTokens.get(i).setWhitespaceBefore(aTokens.get(i - 1).isWhitespace());
    aTokens.get(i).setStartPos(aTokens.get(i).getStartPos() + posFix);
    if (!softHyphenTokens.isEmpty()) {
      if (softHyphenTokens.get(i) != null) {
        aTokens.get(i).addReading(language.getTagger().createToken(softHyphenTokens.get(i),null));
        posFix+=softHyphenTokens.get(i).length() - aTokens.get(i).getToken().length();
      }
    }
  }
  AnalyzedTokenReadings[] tokenArray=new AnalyzedTokenReadings[tokens.size() + 1];
  AnalyzedToken[] startTokenArray=new AnalyzedToken[1];
  int toArrayCount=0;
  AnalyzedToken sentenceStartToken=new AnalyzedToken("",SENTENCE_START_TAGNAME,null);
  startTokenArray[0]=sentenceStartToken;
  tokenArray[toArrayCount++]=new AnalyzedTokenReadings(startTokenArray,0);
  int startPos=0;
  for (  AnalyzedTokenReadings posTag : aTokens) {
    posTag.setStartPos(startPos);
    tokenArray[toArrayCount++]=posTag;
    startPos+=posTag.getToken().length();
  }
  int lastToken=toArrayCount - 1;
  for (int i=0; i < toArrayCount - 1; i++) {
    if (!tokenArray[lastToken - i].isWhitespace()) {
      lastToken-=i;
      break;
    }
  }
  tokenArray[lastToken].setSentEnd();
  if (tokenArray.length == lastToken + 1 && tokenArray[lastToken].isLinebreak()) {
    tokenArray[lastToken].setParagraphEnd();
  }
  return new AnalyzedSentence(tokenArray);
}
