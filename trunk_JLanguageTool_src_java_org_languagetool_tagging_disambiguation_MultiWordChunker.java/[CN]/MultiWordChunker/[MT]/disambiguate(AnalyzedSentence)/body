{
  lazyInit();
  final AnalyzedTokenReadings[] anTokens=input.getTokens();
  final AnalyzedTokenReadings[] output=anTokens;
  for (int i=0; i < anTokens.length; i++) {
    final String tok=output[i].getToken();
    final StringBuilder tokens=new StringBuilder();
    int finalLen=0;
    if (mStartSpace.containsKey(tok)) {
      final int len=mStartSpace.get(tok);
      int j=i;
      int lenCounter=0;
      while (j < anTokens.length) {
        if (!anTokens[j].isWhitespace()) {
          tokens.append(anTokens[j].getToken());
          if (mFull.containsKey(tokens.toString())) {
            final AnalyzedToken tokenStart=new AnalyzedToken(tok,"<" + mFull.get(tokens.toString()) + ">",tokens.toString());
            String oldReading=output[i].toString();
            output[i].addReading(tokenStart);
            output[i].setHistoricalAnnotations("MULTIWORD_CHUNKER" + ": " + oldReading + " -> "+ output[i].toString());
            final AnalyzedToken tokenEnd=new AnalyzedToken(anTokens[finalLen].getToken(),"</" + mFull.get(tokens.toString()) + ">",tokens.toString());
            oldReading=output[finalLen].toString();
            final String prevAnot=output[finalLen].getHistoricalAnnotations();
            output[finalLen].addReading(tokenEnd);
            output[finalLen].setHistoricalAnnotations(prevAnot + "\nMULTIWORD_CHUNKER" + ": "+ oldReading+ " -> "+ output[i].toString());
          }
          lenCounter++;
          if (lenCounter == len) {
            break;
          }
          tokens.append(' ');
        }
        j++;
        finalLen=j;
      }
    }
    if (mStartNoSpace.containsKey(tok)) {
      final int len=mStartNoSpace.get(tok);
      if (i + len <= anTokens.length) {
        for (int j=i; j < i + len; j++) {
          tokens.append(anTokens[j].getToken());
          if (mFull.containsKey(tokens.toString())) {
            final AnalyzedToken tokenStart=new AnalyzedToken(tok,"<" + mFull.get(tokens.toString()) + ">",tokens.toString());
            String oldReading=output[i].toString();
            output[i].addReading(tokenStart);
            output[i].setHistoricalAnnotations("MULTIWORD_CHUNKER" + ": " + oldReading + " -> "+ output[i].toString());
            final AnalyzedToken tokenEnd=new AnalyzedToken(anTokens[i + len - 1].getToken(),"</" + mFull.get(tokens.toString()) + ">",tokens.toString());
            oldReading=output[i + len - 1].toString();
            final String prevAnot=output[i + len - 1].getHistoricalAnnotations();
            output[i + len - 1].addReading(tokenEnd);
            output[i + len - 1].setHistoricalAnnotations(prevAnot + "\nMULTIWORD_CHUNKER" + ": "+ oldReading+ " -> "+ output[i].toString());
          }
        }
      }
    }
  }
  return new AnalyzedSentence(output);
}
