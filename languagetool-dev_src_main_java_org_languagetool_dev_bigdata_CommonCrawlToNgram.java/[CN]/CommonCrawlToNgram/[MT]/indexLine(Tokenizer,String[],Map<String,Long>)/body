{
  for (  String line : lines) {
    List<String> tokens=wordTokenizer.tokenize(line);
    String prevPrev=null;
    String prev=null;
    for (    String token : tokens) {
      if (token.trim().isEmpty()) {
        continue;
      }
      if (prevPrev != null && prev != null) {
        String ngram=prevPrev + " " + prev+ " "+ token;
        Long count=ngramToCount.get(ngram);
        if (count == null) {
          ngramToCount.put(ngram,1L);
        }
 else {
          ngramToCount.put(ngram,count + 1);
        }
        if (ngramToCount.size() > limit) {
          writeToLucene(ngramToCount);
          ngramToCount.clear();
        }
      }
      prevPrev=prev;
      prev=token;
    }
  }
  writeToLucene(ngramToCount);
}
