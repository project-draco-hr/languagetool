{
  List<GoogleToken> newTokens=GoogleToken.getGoogleTokens(term,false,getGoogleStyleWordTokenizer());
  Probability ngram3Left;
  Probability ngram3Middle;
  Probability ngram3Right;
  if (newTokens.size() == 1) {
    ngram3Left=getPseudoProbability(getContext(token,tokens,term,0,2));
    ngram3Middle=getPseudoProbability(getContext(token,tokens,term,1,1));
    ngram3Right=getPseudoProbability(getContext(token,tokens,term,2,0));
  }
 else   if (newTokens.size() == 2) {
    ngram3Left=getPseudoProbability(getContext(token,tokens,newTokens,0,1));
    ngram3Right=getPseudoProbability(getContext(token,tokens,newTokens,1,0));
    ngram3Middle=new Probability((ngram3Left.getProb() + ngram3Right.getProb()) / 2,1.0f);
  }
 else {
    throw new RuntimeException("Words that consists of more than 2 tokens (according to Google tokenization) are not supported yet: " + term + " -> "+ newTokens);
  }
  if (ngram3Left.getCoverage() < MIN_COVERAGE && ngram3Middle.getCoverage() < MIN_COVERAGE && ngram3Right.getCoverage() < MIN_COVERAGE) {
    debug("  Min coverage of %.2f not reached: %.2f, %.2f, %.2f, assuming p=0\n",MIN_COVERAGE,ngram3Left.getCoverage(),ngram3Middle.getCoverage(),ngram3Right.getCoverage());
    return 0.0;
  }
 else {
    return ngram3Left.getProb() * ngram3Middle.getProb() * ngram3Right.getProb();
  }
}
