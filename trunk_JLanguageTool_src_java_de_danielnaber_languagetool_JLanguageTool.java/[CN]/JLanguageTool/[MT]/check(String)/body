{
  SentenceTokenizer sTokenizer=new SentenceTokenizer();
  List sentences=sTokenizer.tokenize(s);
  List ruleMatches=new ArrayList();
  List allRules=getAllRules();
  print(allRules.size() + " rules activated for language " + language);
  int tokenCount=0;
  for (Iterator iter=sentences.iterator(); iter.hasNext(); ) {
    String sentence=(String)iter.next();
    AnalyzedSentence analyzedText=getAnalyzedText(sentence);
    print(analyzedText.toString());
    for (Iterator iterator=allRules.iterator(); iterator.hasNext(); ) {
      Rule rule=(Rule)iterator.next();
      if (disabledRules.contains(rule.getId()))       continue;
      RuleMatch[] thisMatches=rule.match(analyzedText);
      for (int i=0; i < thisMatches.length; i++) {
        RuleMatch thisMatch=new RuleMatch(thisMatches[i].getRule(),thisMatches[i].getFromPos() + tokenCount,thisMatches[i].getToPos() + tokenCount,thisMatches[i].getMessage());
        ruleMatches.add(thisMatch);
      }
    }
    tokenCount+=sentence.length();
  }
  return ruleMatches;
}
