{
  String s=readFile(filename);
  SentenceTokenizer sTokenizer=new SentenceTokenizer();
  WordTokenizer wtokenizer=new WordTokenizer();
  List sentences=sTokenizer.tokenize(s);
  List ruleMatches=new ArrayList();
  List allRules=getallRules();
  int tokenCount=0;
  for (Iterator iter=sentences.iterator(); iter.hasNext(); ) {
    String sentence=(String)iter.next();
    List tokens=wtokenizer.tokenize(sentence);
    AnalyzedToken[] tokenArray=new AnalyzedToken[tokens.size()];
    int toArrayCount=0;
    int startPos=0;
    for (Iterator iterator=tokens.iterator(); iterator.hasNext(); ) {
      String tokenStr=(String)iterator.next();
      tokenArray[toArrayCount]=new AnalyzedToken(tokenStr,"FIXME",startPos);
      toArrayCount++;
      startPos+=tokenStr.length();
    }
    AnalyzedSentence analyzedText=new AnalyzedSentence(tokenArray);
    for (Iterator iterator=allRules.iterator(); iterator.hasNext(); ) {
      Rule rule=(Rule)iterator.next();
      if (disabledRules.contains(rule.getId()))       continue;
      RuleMatch[] thisMatches=rule.match(analyzedText);
      for (int i=0; i < thisMatches.length; i++) {
        RuleMatch thisMatch=new RuleMatch(thisMatches[i].getRule(),thisMatches[i].getFromPos() + tokenCount,thisMatches[i].getToPos() + tokenCount,thisMatches[i].getMessage());
        ruleMatches.add(thisMatch);
      }
    }
    tokenCount+=sentence.length();
  }
  return ruleMatches;
}
