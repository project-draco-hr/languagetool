{
  String s=readFile(filename);
  SentenceTokenizer sTokenizer=new SentenceTokenizer();
  WordTokenizer wtokenizer=new WordTokenizer();
  List sentences=sTokenizer.tokenize(s);
  List ruleMatches=new ArrayList();
  List allRules=getallRules();
  int tokenCount=0;
  for (Iterator iter=sentences.iterator(); iter.hasNext(); ) {
    String sentence=(String)iter.next();
    List tokens=wtokenizer.tokenize(sentence);
    AnalyzedSentence analyzedText=new AnalyzedSentence(tokens);
    for (Iterator iterator=allRules.iterator(); iterator.hasNext(); ) {
      Rule rule=(Rule)iterator.next();
      if (disabledRules.contains(rule.getId()))       continue;
      RuleMatch[] thisMatches=rule.match(analyzedText);
      for (int i=0; i < thisMatches.length; i++) {
        System.err.println("OLDMATCH=" + thisMatches[i]);
        RuleMatch thisMatch=new RuleMatch(thisMatches[i].getRule(),thisMatches[i].getFromPos() + tokenCount,thisMatches[i].getToPos() + tokenCount,thisMatches[i].getMessage());
        System.err.println("NEWMATCH=" + thisMatch);
        ruleMatches.add(thisMatch);
      }
    }
    tokenCount+=sentence.length();
  }
  return ruleMatches;
}
