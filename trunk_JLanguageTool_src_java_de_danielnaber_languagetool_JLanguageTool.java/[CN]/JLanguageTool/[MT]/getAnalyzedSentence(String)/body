{
  final List<String> tokens=wordTokenizer.tokenize(sentence);
  final List<AnalyzedTokenReadings> aTokens=tagger.tag(tokens);
  int numTokens=aTokens.size();
  for (int i=1; i < numTokens; i++) {
    aTokens.get(i).setWhitespaceBefore(aTokens.get(i - 1).isWhitespace());
  }
  final AnalyzedTokenReadings[] tokenArray=new AnalyzedTokenReadings[tokens.size() + 1];
  final AnalyzedToken[] startTokenArray=new AnalyzedToken[1];
  int toArrayCount=0;
  final AnalyzedToken sentenceStartToken=new AnalyzedToken("",SENTENCE_START_TAGNAME,0);
  startTokenArray[0]=sentenceStartToken;
  tokenArray[toArrayCount++]=new AnalyzedTokenReadings(startTokenArray);
  int startPos=0;
  for (  final AnalyzedTokenReadings posTag : aTokens) {
    posTag.setStartPos(startPos);
    tokenArray[toArrayCount++]=posTag;
    startPos+=posTag.getToken().length();
  }
  int lastToken=toArrayCount - 1;
  for (int i=0; i < toArrayCount - 1; i++) {
    if (!tokenArray[lastToken - i].isWhitespace()) {
      lastToken-=i;
      break;
    }
  }
  final AnalyzedToken sentenceEnd=new AnalyzedToken(tokenArray[lastToken].getToken(),SENTENCE_END_TAGNAME,tokenArray[lastToken].getAnalyzedToken(0).getLemma(),tokenArray[lastToken].getAnalyzedToken(0).getStartPos());
  tokenArray[lastToken].addReading(sentenceEnd);
  if (tokenArray.length == 2 && (tokenArray[0].isSentStart()) && tokenArray[1].getToken().equals("\n")) {
    final AnalyzedToken paragraphEnd=new AnalyzedToken(tokenArray[lastToken].getToken(),PARAGRAPH_END_TAGNAME,tokenArray[lastToken].getAnalyzedToken(0).getLemma(),tokenArray[lastToken].getAnalyzedToken(0).getStartPos());
    tokenArray[lastToken].addReading(paragraphEnd);
  }
  AnalyzedSentence finalSentence=new AnalyzedSentence(tokenArray);
  finalSentence=disambiguator.disambiguate(finalSentence);
  return finalSentence;
}
