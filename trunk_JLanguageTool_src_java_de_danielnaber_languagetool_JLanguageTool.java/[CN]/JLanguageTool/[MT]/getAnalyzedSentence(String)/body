{
  List<String> tokens=wordTokenizer.tokenize(sentence);
  List<String> noWhitespaceTokens=new ArrayList<String>();
  for (  String token : tokens) {
    if (isWord(token)) {
      noWhitespaceTokens.add(token);
    }
  }
  List<AnalyzedTokenReadings> aTokens=tagger.tag(noWhitespaceTokens);
  AnalyzedTokenReadings[] tokenArray=new AnalyzedTokenReadings[tokens.size() + 1];
  AnalyzedToken[] startTokenArray=new AnalyzedToken[1];
  int toArrayCount=0;
  AnalyzedToken sentenceStartToken=new AnalyzedToken("",SENTENCE_START_TAGNAME,0);
  startTokenArray[0]=sentenceStartToken;
  tokenArray[toArrayCount++]=new AnalyzedTokenReadings(startTokenArray);
  int startPos=0;
  int noWhitespaceCount=0;
  for (  String tokenStr : tokens) {
    AnalyzedTokenReadings posTag=null;
    if (isWord(tokenStr)) {
      posTag=(AnalyzedTokenReadings)aTokens.get(noWhitespaceCount);
      posTag.startPos=startPos;
      noWhitespaceCount++;
    }
 else {
      posTag=(AnalyzedTokenReadings)tagger.createNullToken(tokenStr,startPos);
    }
    tokenArray[toArrayCount++]=posTag;
    startPos+=tokenStr.length();
  }
  int lastToken=toArrayCount - 1;
  AnalyzedToken sentenceEnd=new AnalyzedToken(tokenArray[lastToken].getToken(),SENTENCE_END_TAGNAME,tokenArray[lastToken].getAnalyzedToken(0).getLemma());
  tokenArray[lastToken].addReading(sentenceEnd);
  if (tokenArray.length == 2) {
    if (tokenArray[0].isSentStart() && tokenArray[1].getToken().equals("\n")) {
      AnalyzedToken paragraphEnd=new AnalyzedToken(tokenArray[lastToken].getToken(),PARAGRAPH_END_TAGNAME,tokenArray[lastToken].getAnalyzedToken(0).getLemma());
      tokenArray[lastToken].addReading(paragraphEnd);
    }
  }
  AnalyzedSentence finalSentence=new AnalyzedSentence(tokenArray);
  finalSentence=disambiguator.disambiguate(finalSentence);
  return finalSentence;
}
