{
  List<String> tokens=wordTokenizer.tokenize(sentence);
  List<AnalyzedTokenReadings> aTokens=tagger.tag(tokens);
  AnalyzedTokenReadings[] tokenArray=new AnalyzedTokenReadings[tokens.size() + 1];
  AnalyzedToken[] startTokenArray=new AnalyzedToken[1];
  int toArrayCount=0;
  AnalyzedToken sentenceStartToken=new AnalyzedToken("",SENTENCE_START_TAGNAME,0);
  startTokenArray[0]=sentenceStartToken;
  tokenArray[toArrayCount++]=new AnalyzedTokenReadings(startTokenArray);
  int startPos=0;
  for (  AnalyzedTokenReadings posTag : aTokens) {
    posTag.startPos=startPos;
    tokenArray[toArrayCount++]=posTag;
    startPos+=posTag.getToken().length();
  }
  int lastToken=toArrayCount - 1;
  for (int i=0; i < lastToken; i++) {
    if (!tokenArray[lastToken - i].getToken().trim().equals("")) {
      lastToken-=i;
      break;
    }
  }
  AnalyzedToken sentenceEnd=new AnalyzedToken(tokenArray[lastToken].getToken(),SENTENCE_END_TAGNAME,tokenArray[lastToken].getAnalyzedToken(0).getLemma(),tokenArray[lastToken].getAnalyzedToken(0).getStartPos());
  tokenArray[lastToken].addReading(sentenceEnd);
  if (tokenArray.length == 2) {
    if (tokenArray[0].isSentStart() && tokenArray[1].getToken().equals("\n")) {
      AnalyzedToken paragraphEnd=new AnalyzedToken(tokenArray[lastToken].getToken(),PARAGRAPH_END_TAGNAME,tokenArray[lastToken].getAnalyzedToken(0).getLemma(),tokenArray[lastToken].getAnalyzedToken(0).getStartPos());
      tokenArray[lastToken].addReading(paragraphEnd);
    }
  }
  AnalyzedSentence finalSentence=new AnalyzedSentence(tokenArray);
  finalSentence=disambiguator.disambiguate(finalSentence);
  return finalSentence;
}
