{
  List tokens=wordTokenizer.tokenize(sentence);
  List noWhitespaceTokens=new ArrayList();
  for (Iterator iterator=tokens.iterator(); iterator.hasNext(); ) {
    String token=(String)iterator.next();
    if (isWord(token)) {
      noWhitespaceTokens.add(token);
    }
  }
  List aTokens=tagger.tag(noWhitespaceTokens);
  AnalyzedToken[] tokenArray=new AnalyzedToken[tokens.size() + 1];
  int toArrayCount=0;
  AnalyzedToken sentenceStartToken=new AnalyzedToken("",SENTENCE_START_TAGNAME,0);
  tokenArray[toArrayCount++]=sentenceStartToken;
  int startPos=0;
  int noWhitespaceCount=0;
  for (Iterator iterator=tokens.iterator(); iterator.hasNext(); ) {
    String tokenStr=(String)iterator.next();
    AnalyzedToken posTag=null;
    if (isWord(tokenStr)) {
      posTag=(AnalyzedToken)aTokens.get(noWhitespaceCount);
      posTag.startPos=startPos;
      noWhitespaceCount++;
    }
 else {
      posTag=(AnalyzedToken)tagger.createNullToken(tokenStr,startPos);
    }
    tokenArray[toArrayCount++]=posTag;
    startPos+=tokenStr.length();
  }
  return new AnalyzedSentence(tokenArray);
}
