{
  WordTokenizer wtokenizer=new WordTokenizer();
  List tokens=wtokenizer.tokenize(sentence);
  List noWhitespaceTokens=new ArrayList();
  for (Iterator iterator=tokens.iterator(); iterator.hasNext(); ) {
    String token=(String)iterator.next();
    if (!token.trim().equals("")) {
      noWhitespaceTokens.add(token);
    }
  }
  List aTokens=tagger.tag(noWhitespaceTokens);
  AnalyzedToken[] tokenArray=new AnalyzedToken[tokens.size() + 1];
  int toArrayCount=0;
  AnalyzedToken sentenceStartToken=new AnalyzedToken("",SENTENCE_START_TAGNAME,0);
  tokenArray[toArrayCount++]=sentenceStartToken;
  int startPos=0;
  int noWhitespaceCount=0;
  for (Iterator iterator=tokens.iterator(); iterator.hasNext(); ) {
    String tokenStr=(String)iterator.next();
    AnalyzedToken posTag=null;
    if (!tokenStr.trim().equals("")) {
      posTag=(AnalyzedToken)aTokens.get(noWhitespaceCount);
      noWhitespaceCount++;
    }
 else {
      posTag=new AnalyzedToken(tokenStr,null,startPos);
    }
    tokenArray[toArrayCount++]=posTag;
    startPos+=tokenStr.length();
  }
  return new AnalyzedSentence(tokenArray);
}
