{
  List<String> tokens=tokenizer.tokenize(input);
  List<String> noWhitespaceTokens=new ArrayList<String>();
  for (  String string : tokens) {
    String token=(String)string;
    if (isWord(token)) {
      noWhitespaceTokens.add(token);
    }
  }
  List<AnalyzedTokenReadings> output=tagger.tag(noWhitespaceTokens);
  StringBuffer outputStr=new StringBuffer();
  for (Iterator<AnalyzedTokenReadings> iter=output.iterator(); iter.hasNext(); ) {
    AnalyzedTokenReadings token=(AnalyzedTokenReadings)iter.next();
    int readingsNumber=token.getReadingsLength();
    for (int j=0; j < readingsNumber; j++) {
      outputStr.append(token.getAnalyzedToken(j).getToken());
      outputStr.append("/[");
      outputStr.append(token.getAnalyzedToken(j).getLemma());
      outputStr.append("]");
      outputStr.append(token.getAnalyzedToken(j).getPOSTag());
      if (readingsNumber > 1 && j < readingsNumber - 1) {
        outputStr.append("|");
      }
    }
    if (iter.hasNext())     outputStr.append(" ");
  }
  TestCase.assertEquals(expected,outputStr.toString());
}
